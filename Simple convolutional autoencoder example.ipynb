{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.utils\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 2   # Dimensiones del espacio latente\n",
    "capacity = 16     # Cantidad de filtros de la primera capa\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ac9cc1c13a4e3d90661a860c21d595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=26421880.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005468544b5047699718866a32055f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=29515.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a29df0832348738974cc10a3f38e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4422102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4e1a22ba644813ac7e7bc8deadba7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5148.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # image = (image - mean) / std\n",
    "])\n",
    "\n",
    "train_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = FashionMNIST(root='./data/FashionMNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 24803\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
    "        self.fc = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        c = capacity\n",
    "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), capacity*2, 7, 7) \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.tanh(self.conv1(x)) \n",
    "        return x\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon\n",
    "    \n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch [1 / 50] average reconstruction error: 0.191237\n",
      "Epoch [2 / 50] average reconstruction error: 0.138373\n",
      "Epoch [3 / 50] average reconstruction error: 0.133182\n",
      "Epoch [4 / 50] average reconstruction error: 0.130021\n",
      "Epoch [5 / 50] average reconstruction error: 0.127302\n",
      "Epoch [6 / 50] average reconstruction error: 0.125241\n",
      "Epoch [7 / 50] average reconstruction error: 0.123620\n",
      "Epoch [8 / 50] average reconstruction error: 0.122437\n",
      "Epoch [9 / 50] average reconstruction error: 0.121454\n",
      "Epoch [10 / 50] average reconstruction error: 0.120735\n",
      "Epoch [11 / 50] average reconstruction error: 0.120104\n",
      "Epoch [12 / 50] average reconstruction error: 0.119652\n",
      "Epoch [13 / 50] average reconstruction error: 0.119163\n",
      "Epoch [14 / 50] average reconstruction error: 0.118691\n",
      "Epoch [15 / 50] average reconstruction error: 0.118370\n",
      "Epoch [16 / 50] average reconstruction error: 0.117936\n",
      "Epoch [17 / 50] average reconstruction error: 0.117677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5a93e5fdbd8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimage_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mimage_batch_recon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch_recon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f261ed2e9d1b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mx_recon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_recon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f261ed2e9d1b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "autoencoder.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "fig = plt.figure()\n",
    "line, = plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Reconstruction error')\n",
    "plt.show()\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_dataloader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "        image_batch_recon = autoencoder(image_batch)\n",
    "        \n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    \n",
    "    line.set_xdata(range(1, epoch+2))\n",
    "    line.set_ydata(train_loss_avg)\n",
    "    \n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
